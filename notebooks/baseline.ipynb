{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from yellowcab.io.input import read_parquet_dataset,read_parquet, read_geo_dataset\n",
    "from yellowcab.cleaning import clean_dataset\n",
    "from yellowcab.io.output import write_parquet\n",
    "from yellowcab.io.utils import get_data_path\n",
    "from yellowcab.preprocessing import transform_columns\n",
    "from tqdm import tqdm\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import calendar\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from imblearn.metrics import classification_report_imbalanced\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "def clean_all_datasets(base_path=get_data_path(), relative_path=\"input/trip_data\",verbose=False):\n",
    "    \"\"\"\n",
    "\n",
    "    :param base_path:\n",
    "    :param relative_path:\n",
    "    :param verbose:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data_path = join(base_path, relative_path)\n",
    "    data_sets = [dataset for dataset in listdir(data_path) if isfile(join(data_path, dataset))]\n",
    "    if not all('.parquet' in name for name in data_sets):\n",
    "        raise ValueError(\"The given directory includes non parquet files\")\n",
    "    for parquet_file in tqdm(data_sets):\n",
    "        #assumes 01.parquet,02.parquet,...\n",
    "        month=int(parquet_file.split('.parquet')[0])\n",
    "        print(f'Started cleaning the {calendar.month_name[month]} data set')\n",
    "        cleaned_df = clean_dataset(read_parquet(parquet_file),month=month,verbose=verbose)\n",
    "        write_parquet(cleaned_df,filename=f'{calendar.month_name[month]}_cleaned.parquet')\n",
    "        print(f'Finished cleaning the {calendar.month_name[month]} data set')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "cab_df = read_parquet_dataset(relative_path=\"input/cleaned\").sample(100,random_state=7)\n",
    "#cab_df = cab_df.loc[~(cab_df['payment_type']=='Unknown')]\n",
    "#Weekend to int"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 100 entries, 2479665 to 19702646\n",
      "Data columns (total 28 columns):\n",
      " #   Column                 Non-Null Count  Dtype         \n",
      "---  ------                 --------------  -----         \n",
      " 0   pickup_datetime        100 non-null    datetime64[ns]\n",
      " 1   dropoff_datetime       100 non-null    datetime64[ns]\n",
      " 2   passenger_count        100 non-null    float64       \n",
      " 3   trip_distance          100 non-null    float64       \n",
      " 4   RatecodeID             100 non-null    object        \n",
      " 5   PULocationID           100 non-null    object        \n",
      " 6   DOLocationID           100 non-null    object        \n",
      " 7   payment_type           100 non-null    object        \n",
      " 8   fare_amount            100 non-null    float64       \n",
      " 9   extra                  100 non-null    float64       \n",
      " 10  mta_tax                100 non-null    float64       \n",
      " 11  tip_amount             100 non-null    float64       \n",
      " 12  tolls_amount           100 non-null    float64       \n",
      " 13  improvement_surcharge  100 non-null    float64       \n",
      " 14  total_amount           100 non-null    float64       \n",
      " 15  congestion_surcharge   100 non-null    float64       \n",
      " 16  centers_lat_pickup     100 non-null    float64       \n",
      " 17  centers_long_pickup    100 non-null    float64       \n",
      " 18  centers_lat_dropoff    100 non-null    float64       \n",
      " 19  centers_long_dropoff   100 non-null    float64       \n",
      " 20  trip_duration_minutes  100 non-null    float64       \n",
      " 21  pickup_month           100 non-null    int64         \n",
      " 22  pickup_day             100 non-null    int64         \n",
      " 23  pickup_hour            100 non-null    int64         \n",
      " 24  dropoff_month          100 non-null    int64         \n",
      " 25  dropoff_day            100 non-null    int64         \n",
      " 26  dropoff_hour           100 non-null    int64         \n",
      " 27  weekend                100 non-null    bool          \n",
      "dtypes: bool(1), datetime64[ns](2), float64(15), int64(6), object(4)\n",
      "memory usage: 22.0+ KB\n"
     ]
    }
   ],
   "source": [
    "cab_df.info()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   pre       rec       spe        f1       geo       iba       sup\n",
      "\n",
      "       Cash       0.67      1.00      0.88      0.80      0.94      0.89         6\n",
      "Credit card       1.00      0.88      1.00      0.93      0.94      0.86        24\n",
      "\n",
      "avg / total       0.93      0.90      0.97      0.91      0.94      0.87        30\n",
      "\n",
      "RMSE:  0.3811561090865957\n",
      "MAE:  0.2806970083816506\n",
      "R2:  0.988808951502751\n"
     ]
    }
   ],
   "source": [
    "def _get_random_state():\n",
    "    \"\"\"\n",
    "    This function defines our random state, so that it's the same in\n",
    "    every application\n",
    "\n",
    "    :return:\n",
    "        int: the integer used for random state paramenter\n",
    "    \"\"\"\n",
    "    RANDOM_STATE = 7\n",
    "    return RANDOM_STATE\n",
    "\n",
    "def _get_column_description_for_prediction():\n",
    "    \"\"\"\n",
    "    This function defines the categories our features belong to.\n",
    "    :return:\n",
    "        dictionary: Dictionary containing our feature categories\n",
    "                    with the associated attributes.\n",
    "    \"\"\"\n",
    "    #why not all features listed?\n",
    "    column_description = {'cyclical_features':['pickup_month','pickup_day','pickup_hour',\n",
    "                                               'dropoff_hour','dropoff_day','dropoff_month'],\n",
    "                      'categorical_features':['RatecodeID', 'payment_type'],\n",
    "                      'temporal_features':['pickup_datetime','dropoff_datetime'],\n",
    "                      'spatial_features':['PULocationID','DOLocationID','centers_lat_dropoff',\n",
    "                                          'centers_long_dropoff','centers_lat_pickup','centers_long_pickup']}\n",
    "    return column_description\n",
    "\n",
    "def _make_data_preparation(df, prediction_type, target):\n",
    "    \"\"\"\n",
    "    TODO: As soon as Simon has corrected weekend in cleaning, line can be removed.\n",
    "    This function reduces the dataframe to one containing only relevant features\n",
    "    for prediction purposes.\n",
    "    :param df (pandas.DataFrame): the given pandas data frame with all initial features\n",
    "           prediction_type (String): Denotes whether used for regression or classification.\n",
    "           :param target: Dependent variable for prediction purposes.\n",
    "    :return: pandas.DataFrame: data frame containing only those features which\n",
    "             are relevant for prediction.\n",
    "\n",
    "    \"\"\"\n",
    "    if prediction_type == \"regression\":\n",
    "        column_description = _get_column_description_for_prediction()\n",
    "        df = transform_columns(df,column_description) #for what transform columns?\n",
    "        df.drop(column_description.get('spatial_features'),inplace=True, axis=1)\n",
    "        df.drop(column_description.get('temporal_features'),inplace=True, axis=1)\n",
    "        df.pop('weekend') # remove after Simons correction\n",
    "    else:\n",
    "        column_description = _get_column_description_for_prediction()\n",
    "        column_description['categorical_features'] = ['RatecodeID'] #payment type should not be transofrmed here\n",
    "        df = transform_columns(df,column_description) #for what transform columns?\n",
    "        df.drop(column_description.get('spatial_features'),inplace=True, axis=1)\n",
    "        df.drop(column_description.get('temporal_features'),inplace=True, axis=1)\n",
    "        df.pop('weekend') # remove after Simons correction\n",
    "\n",
    "    #if prediction_type == \"regression\":\n",
    "    #    df = pd.get_dummies(df)\n",
    "    return df\n",
    "\n",
    "def _make_pipeline(model,scaler_type, numerical_features, model_name, use_sampler=False, sampler=None):\n",
    "    \"\"\"\n",
    "    This function assembles several steps that can be cross-validated together\n",
    "    while setting different parameters.\n",
    "    :param prediction_type (String):\n",
    "        Denotes whether used for regression or classification.\n",
    "    :param use_sampler:\n",
    "    :param model: Used model for prediction.\n",
    "    :return: Pipeline: Sequentially applies the list of transforms\n",
    "             and a final estimator.\n",
    "    \"\"\"\n",
    "    steps = []\n",
    "    preprocessing_steps = []\n",
    "\n",
    "    if scaler_type == 'Robust':\n",
    "        scaler = (\"robust_scaler\", RobustScaler()) #with outlier detection on top\n",
    "        preprocessing_steps.append(scaler)\n",
    "    else:\n",
    "        scaler = (\"standard_scaler\", StandardScaler())\n",
    "        preprocessing_steps.append(scaler)\n",
    "\n",
    "    preprocessor_pipeline= Pipeline(steps=preprocessing_steps)\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[('num', preprocessor_pipeline, numerical_features)])\n",
    "    preprocessor = (\"preprocessor\", preprocessor_pipeline)\n",
    "    steps.append(preprocessor)\n",
    "    if use_sampler:\n",
    "        steps.append(sampler)\n",
    "    prediction_model=(model_name,model)\n",
    "    steps.append(prediction_model)\n",
    "    return Pipeline(steps=steps)\n",
    "\n",
    "def _make_train_test_split(df,target):\n",
    "    \"\"\"\n",
    "    This function splits the input data set into a train an a test set, each\n",
    "    for the regressors X and the dependent variable y.\n",
    "    :param df (pandas.DataFrame): the given pandas data frame containing data which\n",
    "                                  need to be split into train and test data sets.\n",
    "    :param target: Dependent variable for prediction purposes.\n",
    "    :return:\n",
    "        X_train (pandas.DataFrame): Regressors used for training a model\n",
    "        X_test (pandas.DataFrame): Regressors used for testing a model\n",
    "        y_train (pandas.Series): Target values for training a model\n",
    "        y_test (pandas.Series): Target values for testing a model\n",
    "    \"\"\"\n",
    "    y = df.pop(target)\n",
    "    X = df\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=_get_random_state())\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def _print_prediction_scores(prediction_type, y_test, X_test, pipeline):\n",
    "    \"\"\"\n",
    "    This function prints the prediction scores for either a regression or\n",
    "    a classification.\n",
    "    :param prediction_type (String): Denotes whether used for regression or classification.\n",
    "    :param y_test (pandas.Series): Target values for testing a model\n",
    "    :param X_test (pandas.DataFrame): Regressors used for testing a model\n",
    "    :param pipeline: Sequentially applies a list of transforms and a final estimator.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if prediction_type == \"classification\":\n",
    "        print(classification_report_imbalanced(y_test, pipeline.predict(X_test)))\n",
    "    else:\n",
    "        print(\"RMSE: \", np.sqrt(metrics.mean_squared_error(y_test, pipeline.predict(X_test))))\n",
    "        print(\"MAE: \", metrics.mean_absolute_error(y_test, pipeline.predict(X_test)))\n",
    "        print(\"R2: \", metrics.r2_score(y_test, pipeline.predict(X_test)))\n",
    "\n",
    "\n",
    "\n",
    "def make_predictions(df, prediction_type, target, model, model_name, scaler_type, use_sampler=False,\n",
    "                     sampler=None):\n",
    "        \"\"\"\n",
    "        This function predicts and prints the prediction scores of a prediction\n",
    "        task dynamically defined by the input parameters.\n",
    "        :param df (pandas.DataFrame): the given pandas data frame containing data\n",
    "                                      used for prediction.\n",
    "        :param prediction_type (String): Denotes whether used for regression or classification.\n",
    "        :param target: Dependent variable for prediction purposes.\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        df = _make_data_preparation(df, prediction_type, target = target)\n",
    "        X_train, X_test, y_train, y_test = _make_train_test_split(df=df,target=target)\n",
    "        numerical_features = df.select_dtypes(include=['float64']).columns\n",
    "        pipeline = _make_pipeline(model = model, model_name = model_name, scaler_type =scaler_type,\n",
    "                                  numerical_features= numerical_features,\n",
    "                                  use_sampler=use_sampler, sampler=sampler) #parameter overloading\n",
    "        pipeline = pipeline.fit(X_train, y_train)\n",
    "        _print_prediction_scores(prediction_type=prediction_type, y_test=y_test, X_test=X_test,\n",
    "                                 pipeline=pipeline)\n",
    "\n",
    "\n",
    "def make_baseline_predictions(df):\n",
    "    \"\"\"\n",
    "    This function performs baseline predictions for both classification and\n",
    "    regression. Those can be used to evaluate fall other machine learning algorithms,\n",
    "    as it provides the required point of comparison.\n",
    "    :param df (pandas.DataFrame): the given pandas data frame containing data\n",
    "                                  used for prediction.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    classification_model = LogisticRegression(multi_class=\"multinomial\", class_weight=\"balanced\",\n",
    "                                              n_jobs=-1, random_state=_get_random_state(), max_iter=5000)\n",
    "    regression_model_1 = LinearRegression()\n",
    "    #regression_model_2 = LinearRegression()\n",
    "    make_predictions(\n",
    "        df=df, prediction_type=\"classification\", target =\"payment_type\", model = classification_model,\n",
    "        scaler_type= \"standard_scaler\", model_name=\"Classification\"\n",
    "    )\n",
    "    make_predictions(\n",
    "        df=df, prediction_type=\"regression\", target =\"trip_distance\", model = regression_model_1,\n",
    "        scaler_type= \"standard_scaler\", model_name=\"Regression 1\"\n",
    "    )\n",
    "\n",
    "make_baseline_predictions(cab_df)\n",
    "\n",
    "\n",
    "# NEAR MISS anstatt classbased balane\n",
    "#save to pickle für alle modelle"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}